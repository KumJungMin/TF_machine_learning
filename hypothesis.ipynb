{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPED5lbQegnuq9WbafbtqnA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KumJungMin/TF_machine_learning/blob/master/hypothesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXL-wuAz5jZN",
        "colab_type": "text"
      },
      "source": [
        "# 1. 텐서플로우 사용법\n",
        "\n",
        "## 1.1 텐서와 그래프 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "249yf-Eb53BU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f512ad8-9fed-4ae1-d721-70d64c0d2b1b"
      },
      "source": [
        "#1. 텐서플로우를 사용하기 위해 텐서플로우 라이브러리 불러오기\n",
        "import tensorflow as tf\n",
        "\n",
        "#2. tf.constant('상수'), 상수를 hello 변수에 저장하는 코드\n",
        "hello = tf.constant('Hello, Tensorflow')\n",
        "print(hello)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'Hello, Tensorflow', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN4C0_Rk6hZw",
        "colab_type": "text"
      },
      "source": [
        "- 텐서플로우는 rank, shape라는 개념을 지닌다.\n",
        "\n",
        "  : rank 란 특정 배열의 차원을 이야기\n",
        "\n",
        "  : shape 이란 특정 배열이 어떻게 생겼는지, 즉 몇개의 요소가 있는지\n",
        "\n",
        "  : d-type은 해당 텐서에 담긴 요소들의 자료형(string, float, int 등)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZAFTxn76oSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "3                                           # rank:0->스칼라    | shape : []\n",
        "[1.0 , 2.0, 3.]                             # rank:1->벡터      | shape : [3]\n",
        "[[1.0 , 2.0, 3.], [1.0 , 2.0, 3.]]          # rank:2->행렬      | shape : [2,3]\n",
        "[[[1.0 , 2.0, 3.0]],[[1.0 , 2.0, 3.]]]      # rank:3->3차원 텐서 | shape : [2,1,3] "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaYHSynw7hr6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a8ae28e-8fab-495b-e222-7fcfb1cc8bd4"
      },
      "source": [
        "#텐서를 사용해, 다양한 연산 수행이 가능\n",
        "a = tf.constant(10)\n",
        "b = tf.constant(32)\n",
        "c = tf.add(a, b)\n",
        "print(c) \n",
        "\n",
        "#tf.Tensor(42, shape=(), dtype=int32)\n",
        "#tf.Tensor(결과값, shape=(), dtype=타입)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(42, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT4MQzGC9AyN",
        "colab_type": "text"
      },
      "source": [
        "- 텐서플로우는 그래프 생성, 실행이 주를 이룬다.\n",
        "\n",
        "- 그러므로, 결과값의 경우 tf.Tensor()형태로 나온다. \n",
        "\n",
        "- 그래프는 텐서들의 연산 모음이다.\n",
        "\n",
        "- 텐서 플로우는 텐서, 텐서의 연산을 먼저 정의하여 그래프를 만들고,\n",
        "  연산을 실행하는 코드를 넣어 원하는 시점에 실제 연산을 수행한다.(자연실행 방식)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arTbiVU79cmw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4b11692c-6cc5-449e-f69a-76a5d1e95ee0"
      },
      "source": [
        "#그래프 실행 : Session 안에서 이루어짐(지원X)\n",
        "# sess = tf.Session()\n",
        "# print(sess.run(hello))\n",
        "# print(sess.run([a,b,c]))\n",
        "# sess.close()\n",
        "\n",
        "#그래프 실행 : 일반 파이썬처럼 가능\n",
        " print((hello))      #tf.Tensor(b'Hello, Tensorflow', shape=(), dtype=string)\n",
        "print(([a,b,c]))      #[10,32,42]\n",
        "#[  <tf.Tensor: shape=(), dtype=int32, numpy=10>, \n",
        "#   <tf.Tensor: shape=(), dtype=int32, numpy=32>, \n",
        "#   <tf.Tensor: shape=(), dtype=int32, numpy=42>  ]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: shape=(), dtype=int32, numpy=10>, <tf.Tensor: shape=(), dtype=int32, numpy=32>, <tf.Tensor: shape=(), dtype=int32, numpy=42>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-0q1oY5-HFN",
        "colab_type": "text"
      },
      "source": [
        "### Session이 없다고 뜬다면?\n",
        "\n",
        "- TF v2.0은 v1.0의 그래프 모드에 대한 Eager 모드를 지원합니다. \n",
        "\n",
        "- 따라서 tf.session ()은 v2.0에서 지원되지 않습니다. \n",
        "\n",
        "- 따라서 Eager 모드에서 작동하도록 코드를 다시 작성하는 것이 좋습니다.\n",
        "\n",
        "<br/>\n",
        "\n",
        "### 이제는 일반 파이썬처럼 즉시 실행이 가능!  \n",
        "\n",
        "텐서플로 1.x에서는 사용자가 tf.* API를 호출해서 추상 구문 트리를 수동으로 구성했습니다. 그다음 session.run()을 호출할 때 출력 텐서와 입력 텐서를 전달하여 추상 구문 트리를 수동으로 컴파일합니다. 텐서플로 2.0은 (보통의 파이썬처럼) 즉시 실행됩니다. 텐서플로 2.0에서 그래프와 세션은 구현 상세(implementation detail)처럼 느껴질 것입니다.\n",
        "\n",
        "즉시 실행으로 인한 부수효과 중 하나는 더이상 tf.control_dependencies()이 필요하지 않다는 것입니다. 모든 코드는 라인 순서대로 실행됩니다(tf.function 안의 코드도 이 효과로 쓰여진 순서대로 실행됩니다).\n",
        "<a href=\"https://www.tensorflow.org/guide/effective_tf2\">공식문서 보기</a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG55Nspi-7PW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d7f3d22b-afaf-4c30-eef2-4811d6f06ce2"
      },
      "source": [
        "#TF1.x hello world :\n",
        "# msg = tf.constant('Hello, TensorFlow!')\n",
        "# sess = tf.Session()\n",
        "# print(sess.run(msg))\n",
        "\n",
        "#TF2.x hello world :\n",
        "msg = tf.constant('Hello, TensorFlow!')\n",
        "tf.print(msg)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello, TensorFlow!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS8TUjhqBr-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-jO0zid_spu",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 플레이스홀더와 변수\n",
        "\n",
        "- 플레이스홀더 : 그래프에 사용할 입력값을 나중에 받기 위해 사용하는 매개변수\n",
        "\n",
        "\n",
        "- 변수 : 그래프를 최적화하는 용도, 학습 함수가 학습한 결과를 갱신하기 위해 쓰는 변수. <br/> 이 변수값에 따라 신경망의 성능이 좌우.\n",
        "\n",
        "### placehoder는 새 버전에서는 지원하지 않습니다. 그러므로...\n",
        "- 아래와 같이 v1을 사용하기 위해 v2를 꺼둡니다.\n",
        "```python\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "x = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n",
        "```\n",
        "\n",
        "- 하지만 다음과 같은 알림말이 뜰 겁니다. 개념 확인을 위해서만 v1을 쓰고 실제 코딩시 사용하지 않는 게 좋습니다.\n",
        "\" is deprecated and will be removed in a future version.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpHazeNFAT0c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "d9cdbbbd-956b-40e3-e207-97e4a6d49417"
      },
      "source": [
        "import tensorflow.compat.v1 as tf_1\n",
        "tf_1.disable_v2_behavior()\n",
        "x = tf_1.placeholder(shape=[None, 2], dtype=tf.float32)\n",
        "\n",
        "# placeholder\n",
        "X = tf_1.placeholder(tf.float32, [None, 3])\n",
        "print(X)\n",
        "\n",
        "#Tensor(\"Placeholder_1:0\", shape=(?, 3), dtype=float32)\n",
        "#(?,3)모양의 float32 자료형을 가진 텐서가 생성"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "Tensor(\"Placeholder_1:0\", shape=(?, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axnsdSrECFb3",
        "colab_type": "text"
      },
      "source": [
        "- 나중에 placeholder X에 넣을 자료를 정의해봅시다.\n",
        "\n",
        "- 앞서 텐서 모양은 (?,3)으로, 두 번째 차원은 요소를 3개씩 가지고 있어야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPd_D68OCVbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = [[1,2,3], [4,5,6]]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWgLbl-gCZ4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#변수 정의\n",
        "#v2(random.normal) - v1(random_normal)\n",
        "#random.normal을 사용해, 정규분포의 무작위 값으로 초기화함\n",
        "w = tf.Variable(tf.random.normal([3,2]))  #[3,2]행렬형태의 텐서 -> ex) [[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]]\n",
        "b= tf.Variable(tf.random.normal([2,1]))   #[2,1]행렬형태의 텐서"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVbDsCKODWWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#입력값과 변수를 계산해보자\n",
        "#w,x는 행렬이므로, tf.matmal함수를 사용\n",
        "#행렬이 아닌경우 -> (*), tf.mul를 사용\n",
        "expr = tf.matmul(X, w) + b"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnbMZ9SjEAKV",
        "colab_type": "text"
      },
      "source": [
        "- 앞선, 두 행렬을 곱하기 위해서는 같아야 하는 게 있다. (AxB)\n",
        "\n",
        "- X[0, a], B[a, 2]처럼 X의열 == B의 행이 같아야함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVpQkyWLEZ0f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "ce585e72-9bdd-4062-f3a5-6db6bb578909"
      },
      "source": [
        "# 앞선 결과들을 실행해보자 v1방법\n",
        "sess = tf_1.Session()\n",
        "\n",
        "#앞에서 정의한 변수를 초기화\n",
        "#기존학습값을 가져오는 게 아닌 새로운 값을 가져오는 것이므로 연산 전 이 함수 필수\n",
        "sess.run(tf_1.global_variables_initializer()) \n",
        "\n",
        "print(\"=== x_data ===\")\n",
        "print(x_data)\n",
        "print(\"=== w ===\")\n",
        "print(w)\n",
        "print(\"=== b ===\")\n",
        "print(b)\n",
        "print(\"=== expr ===\")\n",
        "\n",
        "#feed_dict()의 매개변수는, 그래프를 실행할 때 사용할 입력값으로 지정\n",
        "print(sess.run(expr, feed_dict={X: x_data}))\n",
        "\n",
        "sess.close()\n",
        "\n",
        "\n",
        "\n",
        "# === x_data ===\n",
        "# [[1, 2, 3], [4, 5, 6]]\n",
        "# === w ===\n",
        "# <tf.Variable 'Variable_2:0' shape=(3, 2) dtype=float32>\n",
        "# === b ===\n",
        "# <tf.Variable 'Variable_3:0' shape=(2, 1) dtype=float32>\n",
        "# === expr(행렬곱[2,3][3,2]+형렬 수식[2,1]) ===\n",
        "# [[ 0.69783604 -2.5531518 ]\n",
        "#  [ 1.4701934  -0.4009331 ]]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== x_data ===\n",
            "[[1, 2, 3], [4, 5, 6]]\n",
            "=== w ===\n",
            "<tf.Variable 'Variable_2:0' shape=(3, 2) dtype=float32>\n",
            "=== b ===\n",
            "<tf.Variable 'Variable_3:0' shape=(2, 1) dtype=float32>\n",
            "=== expr ===\n",
            "[[ 0.69783604 -2.5531518 ]\n",
            " [ 1.4701934  -0.4009331 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnscdrIkFoyV",
        "colab_type": "text"
      },
      "source": [
        "- expr수식에는 X,w,b를 사용했다.\n",
        "\n",
        "- X는 placeholder라 X에 값을 넣지 않으면 오류가 생긴다. \n",
        "\n",
        "- 따라서 미리 정의한 x_data를 X값으로 넣어준다. {X : x_data}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLx0QI32GG_k",
        "colab_type": "text"
      },
      "source": [
        "## 1. 선형회귀모델 구현하기\n",
        "\n",
        "- 선형회귀란, 주어진 x,y값을 가지고 서로 간의 관계를 파악하는 것이다.\n",
        "\n",
        "- 이 관계를 알게되면 새로운 x가 주어졌을 때 y값을 쉽게 알 수 있다.\n",
        "\n",
        "- 어떤 입력에 대한 출력을 예측하는 것, 이것이 머신러닝의 기초이다.\n",
        "\n",
        "- 텐서플로의 최적화 함수를 이용해, x와 y의 상관관계를 분석하는 선형회귀모델을 실행해보자.\n",
        "\n",
        "- 아래 코드는 주어진 x_data와 y_data의 상관관계를 파악하고자 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRkpFcQXG0w-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x,y의 상관관계를 설명하기 위한 변수인 W, b를 각각 -1.0~1.0사이의 균등분포를 가진 무작위 값으로 초기화한다.\n",
        "\n",
        "W = tf.Variable(tf.random.uniform([1], -1.0, 1.0))   #uniform(shape, start, end)\n",
        "b = tf.Variable(tf.random.uniform([1], -1.0, 1.0))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nICy2wG9OpL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = [1,2,3]\n",
        "y_data = [1,2,3]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMMI4mu2JSi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#자료를 입력받을 placeholder를 설정한다.\n",
        "X = tf_1.placeholder(tf.float32, name=\"X\")  #shape, name\n",
        "Y = tf_1.placeholder(tf.float32, name=\"Y\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfq3LBesKNZU",
        "colab_type": "text"
      },
      "source": [
        "- x,y의 상관관계(선형관계)를 분석하기 위한 수식을 아래 코드에 작성한다.\n",
        "\n",
        "- W와의 곱과 b의 합을 통해 -> X,Y의 관계를 설명한다는 말이다.\n",
        "\n",
        "- X가 주어졌을 때 -> Y를 만들어 낼 수 있는 W, b를 찾아내겠다는 의미\n",
        "\n",
        "- W, X는 행렬이 아니므로 tf.matmal이 아닌 기본 곱셈 연산자를 쓴다.\n",
        "\n",
        "- 가중치(w)는 입력이 결과에 주는 영향을 조절하는 매개변수\n",
        "\n",
        "- 편향(b)은 노드(x)가 얼마나 쉽게 활성화(1로 출력; activation)되느냐를 조정하는 매개변수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw7JyGS8JnyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hypothsis = W*X + b     #W(가중치), b(편향)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6bBiHGuK2xq",
        "colab_type": "text"
      },
      "source": [
        "### 손실함수\n",
        "\n",
        ": 한 쌍(x,y)의 데이터에 대한 손실값을 계산하는 함수이다.\n",
        "\n",
        ": 손실값은 실제값과 모델로 예측한 값이 얼마나 차이가 나는가를 나타내는 값이다.\n",
        "\n",
        "<br/>\n",
        "\n",
        ": 손실값이 작을수록 그 모델이 X,Y의 관계를 잘 설명하고 있다는 뜻이다.\n",
        "\n",
        ": 손실값이 작을수록 주어진 X값에 대한 Y값을 정확하게 예측할 수 있다는 뜻이다.\n",
        "\n",
        ": 이러한 손실을 전체데이터에 대해 구한 경우, 이를 비용이라고 한다.\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        ": 즉, 학습이란 변수들의 값을 다양하게 넣어 계산하면서 -> 이러한 손실을 최소화하는 W, b를 구하는 것\n",
        "\n",
        ": 손실값으로, (예측값과 실제값의 거리)를 자주 사용\n",
        "\n",
        ": 우리는 손실값을 (예측값-실제값)^2 하고, 비용은 모든 데이터에 대한 손실값의 평균으로 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "desrO2hFL3u6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cost = tf.reduce_mean(tf.square(hypothsis - Y))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkrTwVYNMJVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#경사하강법최적화함수를 이용해 -> 손실값을 최소하는 연산 그래프를 생성\n",
        "optimizer = tf_1.train.GradientDescentOptimizer(learning_rate=0.1)  #v1 - GradientDescentOptimizer\n",
        "train_op = optimizer.minimize(cost)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx90At2EMitJ",
        "colab_type": "text"
      },
      "source": [
        "### 최적화 함수?\n",
        "\n",
        "- 가중치와 편향 값을 변경해주면서, 손실값을 최소화하는 가장 최적화된 가중치와 편향값을 찾아주는 함수이다.\n",
        "\n",
        "- 최적화를 빠르게 하기 위해 여러가지 방법을 사용하는데 경사하강법이 제일 기본적인 알고리즘이다\n",
        "\n",
        "- 경사하강법은 함수의 기울기를 구하고 -> 기울기가 낮은 쪽으로 계속 이동하면서 -> 최적의 값을 찾아 나가는 것이다.\n",
        "\n",
        "<br/>\n",
        "\n",
        "- 최적화 함수의 학습률(learning_rate)는 학습을 얼마나 급하게 할 건지 설정하는 값이다.\n",
        "\n",
        "- 학습률이 너무 크면, 최적의 손실값을 차지 못하고 지나치게 된다.\n",
        "\n",
        "- 학습률이 너무 작으면, 학습 속도가 매우 느려진다.\n",
        "\n",
        "- 이처럼, 학습을 하는 과정에 영향을 주는 변수를 \"하이퍼파라미터\"라고 한다.\n",
        "\n",
        "<br/>\n",
        "\n",
        "- 하이퍼파라미터값에 따라 학습 속도, 신경망 성능이 크게 달라진다.\n",
        "\n",
        "- 머신러닝에서는 하이퍼파라미터를 잘 튜닝하는 게 큰 과제이다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE7ADBbbNgma",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "45cca60f-15eb-4762-c42b-c69f09440f4d"
      },
      "source": [
        "#변수 초기화\n",
        "#With 기능을 사용해 세션 블록을 만들고 자동처리되게 한다.\n",
        "\n",
        "with tf_1.Session() as sess:\n",
        "  sess.run(tf_1.global_variables_initializer())\n",
        "\n",
        "\n",
        "  #최적화를 수행하는 train_op 그래프 실행, 실행시마다 변화하는 손실값을 출력하게 함\n",
        "  #학습을 100번 수행하며, feed_dict매개변수를 통해 -> 상관관계를 알아내고자 하는 x_data, y_data를 입력\n",
        "  for step in range(100):\n",
        "    _, cost_val = sess.run([train_op, cost], feed_dict={X:x_data, Y:y_data})\n",
        "    print(step, cost_val, sess.run(W), sess.run(b))\n",
        "\n",
        "  \n",
        "  #실행해보기\n",
        "  print(\"X:5 Y:\", sess.run(hypothsis,feed_dict={X:5}))    #x가 5일때 -> y예측값\n",
        "  print(\"X:2.5, Y\", sess.run(hypothsis,feed_dict={X:2.5}))#x가 2.5일때 -> y예측값\n",
        "\n",
        "  # X:5 Y: [4.9159107]  \n",
        "  # X:2.5, Y [2.493007]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 15.555412 [0.8240467] [0.85319895]\n",
            "1 0.27193367 [0.6469902] [0.7529405]\n",
            "2 0.08527886 [0.6752898] [0.7435563]\n",
            "3 0.07915274 [0.68093014] [0.7247291]\n",
            "4 0.0753681 [0.68883705] [0.7074112]\n",
            "5 0.071787775 [0.69629127] [0.69039416]\n",
            "6 0.068377785 [0.70359504] [0.6737988]\n",
            "7 0.06512982 [0.7107202] [0.657601]\n",
            "8 0.062036086 [0.71767426] [0.6417927]\n",
            "9 0.059089333 [0.7244612] [0.62636447]\n",
            "10 0.05628252 [0.73108494] [0.6113071]\n",
            "11 0.05360904 [0.7375495] [0.5966117]\n",
            "12 0.051062595 [0.74385864] [0.58226955]\n",
            "13 0.048637077 [0.75001603] [0.5682722]\n",
            "14 0.046326797 [0.75602555] [0.5546113]\n",
            "15 0.044126213 [0.7618905] [0.54127884]\n",
            "16 0.04203018 [0.7676145] [0.5282669]\n",
            "17 0.04003373 [0.77320087] [0.5155677]\n",
            "18 0.03813211 [0.77865297] [0.5031738]\n",
            "19 0.036320787 [0.783974] [0.49107787]\n",
            "20 0.034595523 [0.7891671] [0.4792727]\n",
            "21 0.032952238 [0.7942354] [0.46775132]\n",
            "22 0.03138699 [0.7991818] [0.4565069]\n",
            "23 0.029896071 [0.8040094] [0.4455328]\n",
            "24 0.028476005 [0.8087208] [0.43482247]\n",
            "25 0.027123341 [0.813319] [0.42436963]\n",
            "26 0.025834978 [0.8178067] [0.4141681]\n",
            "27 0.024607798 [0.8221865] [0.4042118]\n",
            "28 0.023438914 [0.826461] [0.3944948]\n",
            "29 0.022325555 [0.8306328] [0.38501143]\n",
            "30 0.021265052 [0.8347043] [0.37575603]\n",
            "31 0.020254966 [0.8386779] [0.36672312]\n",
            "32 0.019292826 [0.84255594] [0.35790735]\n",
            "33 0.01837642 [0.8463408] [0.3493035]\n",
            "34 0.017503506 [0.85003465] [0.3409065]\n",
            "35 0.016672093 [0.8536397] [0.33271137]\n",
            "36 0.015880136 [0.85715806] [0.3247132]\n",
            "37 0.015125845 [0.86059195] [0.31690735]\n",
            "38 0.014407336 [0.86394316] [0.3092891]\n",
            "39 0.013722982 [0.86721385] [0.30185398]\n",
            "40 0.013071135 [0.870406] [0.29459766]\n",
            "41 0.012450252 [0.8735213] [0.28751573]\n",
            "42 0.011858858 [0.8765618] [0.28060406]\n",
            "43 0.011295546 [0.8795292] [0.27385852]\n",
            "44 0.010758996 [0.88242525] [0.26727515]\n",
            "45 0.010247947 [0.88525164] [0.26085004]\n",
            "46 0.009761155 [0.8880101] [0.25457937]\n",
            "47 0.009297494 [0.89070225] [0.24845946]\n",
            "48 0.00885586 [0.89332974] [0.24248669]\n",
            "49 0.008435191 [0.89589393] [0.23665744]\n",
            "50 0.008034531 [0.8983967] [0.23096839]\n",
            "51 0.0076528727 [0.9008391] [0.22541603]\n",
            "52 0.007289359 [0.90322286] [0.2199972]\n",
            "53 0.006943107 [0.9055493] [0.21470861]\n",
            "54 0.0066132997 [0.90781987] [0.20954718]\n",
            "55 0.006299156 [0.9100357] [0.20450978]\n",
            "56 0.0059999432 [0.9121985] [0.19959353]\n",
            "57 0.005714945 [0.9143092] [0.19479544]\n",
            "58 0.0054434887 [0.91636914] [0.19011268]\n",
            "59 0.005184918 [0.91837955] [0.1855425]\n",
            "60 0.0049386225 [0.9203416] [0.18108217]\n",
            "61 0.004704029 [0.9222565] [0.17672908]\n",
            "62 0.0044806 [0.9241255] [0.17248066]\n",
            "63 0.004267762 [0.92594945] [0.16833434]\n",
            "64 0.004065048 [0.9277296] [0.1642877]\n",
            "65 0.003871944 [0.92946684] [0.16033831]\n",
            "66 0.0036880197 [0.9311624] [0.15648389]\n",
            "67 0.003512839 [0.9328172] [0.15272214]\n",
            "68 0.0033459805 [0.9344323] [0.14905082]\n",
            "69 0.003187043 [0.9360085] [0.14546773]\n",
            "70 0.0030356515 [0.9375468] [0.14197078]\n",
            "71 0.0028914583 [0.9390481] [0.1385579]\n",
            "72 0.0027541092 [0.9405134] [0.13522708]\n",
            "73 0.0026232975 [0.9419434] [0.13197632]\n",
            "74 0.0024986858 [0.943339] [0.12880369]\n",
            "75 0.0023799932 [0.9447011] [0.12570733]\n",
            "76 0.0022669432 [0.9460305] [0.12268543]\n",
            "77 0.002159265 [0.9473279] [0.11973616]\n",
            "78 0.0020566906 [0.94859403] [0.11685776]\n",
            "79 0.0019589989 [0.9498299] [0.11404861]\n",
            "80 0.0018659447 [0.95103586] [0.11130692]\n",
            "81 0.0017773152 [0.952213] [0.1086312]\n",
            "82 0.001692892 [0.95336175] [0.10601977]\n",
            "83 0.0016124742 [0.9544829] [0.10347112]\n",
            "84 0.0015358863 [0.95557714] [0.10098375]\n",
            "85 0.0014629235 [0.95664495] [0.09855613]\n",
            "86 0.0013934377 [0.95768726] [0.09618694]\n",
            "87 0.0013272512 [0.9587044] [0.09387466]\n",
            "88 0.0012641997 [0.9596971] [0.09161796]\n",
            "89 0.0012041504 [0.96066594] [0.08941551]\n",
            "90 0.0011469527 [0.9616115] [0.08726603]\n",
            "91 0.0010924697 [0.96253437] [0.08516823]\n",
            "92 0.0010405794 [0.96343505] [0.08312085]\n",
            "93 0.000991151 [0.96431404] [0.08112267]\n",
            "94 0.0009440699 [0.9651719] [0.07917251]\n",
            "95 0.00089922315 [0.9660091] [0.07726924]\n",
            "96 0.00085651083 [0.96682626] [0.07541176]\n",
            "97 0.00081582315 [0.9676237] [0.07359891]\n",
            "98 0.0007770728 [0.968402] [0.07182965]\n",
            "99 0.0007401619 [0.9691616] [0.07010291]\n",
            "X:5 Y: [4.9159107]\n",
            "X:2.5, Y [2.493007]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}